{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sindhu213/Pytorch/blob/main/NLP/rnn_language_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LcpNlmMyBiu4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9eb25a14-a9c4-4717-bfb9-930e8661e176"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\",force_remount=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import math\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from torch import nn, Tensor\n",
        "from torchtext.vocab import vocab\n",
        "from collections import Counter,OrderedDict\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "il59hVX1CbVb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Preprocessing"
      ],
      "metadata": {
        "id": "XltwOBFCJJbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_dir = Path('drive/My Drive/assets/AndThenThereWereNone.txt')\n",
        "with open(file_dir, 'r') as file:\n",
        "  TEXT = file.read()"
      ],
      "metadata": {
        "id": "17g7fFrNCzWP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tokenizer(sentence):\n",
        "  tokenized = re.sub(r'[^\\w\\s]+',' ',sentence.lower())\n",
        "  return tokenized.split()"
      ],
      "metadata": {
        "id": "fNb7KpSDKuu7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counter = Counter(get_tokenizer(TEXT))\n",
        "sorted_by_freq = sorted(counter.items(),key=lambda x: x[1], reverse=True)\n",
        "ordered_dict = OrderedDict(sorted_by_freq)\n",
        "\n",
        "Vocab = vocab(ordered_dict,min_freq=1)    \n",
        "Vocab.set_default_index(0)"
      ],
      "metadata": {
        "id": "WNaS6QaYef6o"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_pipeline = lambda x: Vocab(get_tokenizer(x))  "
      ],
      "metadata": {
        "id": "lFWsUZlKIgd4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_dataset(input:str,seq_length:int,stride:int):\n",
        "  container = []\n",
        "  input_encoded = text_pipeline(input)\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  \n",
        "  for i in range(0,len(input_encoded)-seq_length,stride):\n",
        "    data_encoded = input_encoded[i:i+seq_length+1]\n",
        "    container.append(torch.tensor(data_encoded,device=device,dtype=torch.int64))\n",
        "  return container"
      ],
      "metadata": {
        "id": "g6OEBbCeJT1O"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "SEQ_LENGTH = 100       \n",
        "BATCH_SIZE = 32\n",
        "STRIDE = 64\n",
        "\n",
        "DATA = build_dataset(TEXT,SEQ_LENGTH,STRIDE)\n",
        "TRAIN_DL = DataLoader(DATA,batch_size=BATCH_SIZE,drop_last=True,shuffle=True)  "
      ],
      "metadata": {
        "id": "vcHphD2k6bFd"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Length of TRAIN_DL: \",len(TRAIN_DL))\n",
        "print(\"Total no. of tokens: \",len(get_tokenizer(TEXT)))\n",
        "print(\"Total no. of unique tokens: \",len(Vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeS0NZhrSQ_d",
        "outputId": "4a54ec7f-cac1-434d-f2a2-6bcf491800e7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of TRAIN_DL:  26\n",
            "Total no. of tokens:  55200\n",
            "Total no. of unique tokens:  5304\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Definition and Initialization"
      ],
      "metadata": {
        "id": "rmwUqha5oAdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LanguageModelling(nn.Module):\n",
        "\n",
        "  def __init__(self,vocab_size,embed_dim,num_layers,rnn_hidden_dim,tie_weights=False):\n",
        "    super().__init__()\n",
        "    self.num_layers = num_layers\n",
        "    self.rnn_hidden_dim = rnn_hidden_dim\n",
        "    self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    self.embedding = nn.Embedding(vocab_size, embed_dim)  \n",
        "    self.lstm = nn.LSTM(embed_dim,rnn_hidden_dim,num_layers,batch_first=True)   \n",
        "    self.fc = nn.Linear(rnn_hidden_dim,vocab_size)\n",
        "    if tie_weights:\n",
        "      assert rnn_hidden_dim == embed_dim, \"rnn_hidden_dim must be equal to embed_dim if tie_weights is set to True.\"\n",
        "      self.fc.weight = self.embedding.weight\n",
        "\n",
        "\n",
        "  def forward(self,input,hidden,cell):\n",
        "    \"\"\"\n",
        "    INPUT:\n",
        "        input: [batch_size,seq_length]\n",
        "        hidden: [num_layers,batch_size,rnn_hidden_dim]\n",
        "        cell: [num_layers,batch_size,rnn_hidden_dim]\n",
        "\n",
        "    RETURNS:\n",
        "        out: [batch_size,seq_length,vocab_size]\n",
        "        hidden: [num_layers,batch_size,rnn_hidden_dim]\n",
        "        cell: [num_layers,batch_size,rnn_hidden_dim]\n",
        "    \"\"\"\n",
        "    # out: [batch_size,seq_length,embed_dim]\n",
        "    out = self.embedding(input) \n",
        "\n",
        "    # out: [batch_size,seq_length,rnn_hidden_dim]\n",
        "    # hidden: [num_layers,batch_size,rnn_hidden_dim]\n",
        "    # cell: [num_layers,batch_size,rnn_hidden_dim]\n",
        "    out,(hidden,cell) = self.lstm(out,(hidden,cell))   \n",
        "\n",
        "    # out: [batch_size,seq_length,vocab_size]\n",
        "    out = self.fc(out)\n",
        "    return out,hidden,cell \n",
        "\n",
        "\n",
        "  def init_hidden_and_cell(self,batch_size):\n",
        "    hidden = torch.zeros(self.num_layers,batch_size,self.rnn_hidden_dim)    \n",
        "    cell = torch.zeros(self.num_layers,batch_size,self.rnn_hidden_dim)\n",
        "    return hidden.to(device), cell.to(device)"
      ],
      "metadata": {
        "id": "gUXheAQVo0nA"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = len(Vocab)\n",
        "EMBED_DIM = 128\n",
        "RNN_HIDDEN_DIM = 128\n",
        "NUM_LAYERS = 2\n",
        "TIE_WEIGHTS = False"
      ],
      "metadata": {
        "id": "hBJ0xK4m6QvC"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = LanguageModelling(VOCAB_SIZE,EMBED_DIM,NUM_LAYERS,RNN_HIDDEN_DIM,TIE_WEIGHTS)\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIT6ZdFMa1rl",
        "outputId": "24eba08b-5c48-4f93-8b23-3b8ef24e4b0a"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LanguageModelling(\n",
              "  (embedding): Embedding(5304, 128)\n",
              "  (lstm): LSTM(128, 128, num_layers=2, batch_first=True)\n",
              "  (fc): Linear(in_features=128, out_features=5304, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "  return sum(param.numel() for param in model.parameters() if param.requires_grad)\n",
        "\n",
        "print(\"Total no. of trainable parameters: {: ,}\".format(count_parameters(model)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6y6F3FkdXuh9",
        "outputId": "3722ff1c-7ee5-4519-8e52-39d935de8519"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total no. of trainable parameters:  1,627,320\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training"
      ],
      "metadata": {
        "id": "CX6yeHEtZ6LH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=0.005) "
      ],
      "metadata": {
        "id": "sfyo6FGhZuq8"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(data_iter):\n",
        "  model.train()\n",
        "  epoch_loss =  0.0\n",
        "  # hidden: [num_layers,batch_size,rnn_hidden_dim]\n",
        "  # cell: [num_layers,batch_size,rnn_hidden_dim]\n",
        "  hidden,cell = model.init_hidden_and_cell(BATCH_SIZE)\n",
        "\n",
        "  # input: [batch_size,seq_length+1]\n",
        "  for input in data_iter:   \n",
        "    optimizer.zero_grad()\n",
        "    hidden = hidden.detach()\n",
        "    cell = cell.detach()\n",
        "    # prediction: [batch_size,seq_length,vocab_size]\n",
        "    # hidden: [num_layers,batch_size,rnn_hidden_dim]\n",
        "    # cell: [num_layers,batch_size,rnn_hidden_dim]\n",
        "    prediction, hidden, cell = model(input[:,:-1],hidden,cell)\n",
        "\n",
        "    # prediction: [batch_size*seq_length,vocab_size]\n",
        "    prediction = prediction.reshape(-1,VOCAB_SIZE)\n",
        "\n",
        "    # label: [batch_size*seq_length]\n",
        "    label = input[:,1:].reshape(-1)\n",
        "\n",
        "    # loss: [batch_size*seq_length]\n",
        "    loss = loss_fn(prediction,label)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    epoch_loss += loss.item()*SEQ_LENGTH\n",
        "      \n",
        "  return epoch_loss/len(data_iter)"
      ],
      "metadata": {
        "id": "nqy2SSuWjTsl"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS = 100\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  training_loss = train(TRAIN_DL)\n",
        "  if epoch%5 == 0: \n",
        "    print(f\"EPOCH: {epoch}\")\n",
        "    print(f\"-------TRAIN_LOSS: {training_loss: .4f}\")"
      ],
      "metadata": {
        "id": "7PkiXLDVJDRC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfe61f8b-f751-4e6a-bb84-e9500b2a0d8c"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 0\n",
            "-------TRAIN_LOSS:  28.8596\n",
            "EPOCH: 5\n",
            "-------TRAIN_LOSS:  23.9844\n",
            "EPOCH: 10\n",
            "-------TRAIN_LOSS:  23.1942\n",
            "EPOCH: 15\n",
            "-------TRAIN_LOSS:  22.6879\n",
            "EPOCH: 20\n",
            "-------TRAIN_LOSS:  21.9664\n",
            "EPOCH: 25\n",
            "-------TRAIN_LOSS:  21.8178\n",
            "EPOCH: 30\n",
            "-------TRAIN_LOSS:  21.3195\n",
            "EPOCH: 35\n",
            "-------TRAIN_LOSS:  20.1657\n",
            "EPOCH: 40\n",
            "-------TRAIN_LOSS:  19.8128\n",
            "EPOCH: 45\n",
            "-------TRAIN_LOSS:  19.4703\n",
            "EPOCH: 50\n",
            "-------TRAIN_LOSS:  20.1425\n",
            "EPOCH: 55\n",
            "-------TRAIN_LOSS:  19.1655\n",
            "EPOCH: 60\n",
            "-------TRAIN_LOSS:  19.6734\n",
            "EPOCH: 65\n",
            "-------TRAIN_LOSS:  17.5920\n",
            "EPOCH: 70\n",
            "-------TRAIN_LOSS:  17.8701\n",
            "EPOCH: 75\n",
            "-------TRAIN_LOSS:  16.9511\n",
            "EPOCH: 80\n",
            "-------TRAIN_LOSS:  17.0411\n",
            "EPOCH: 85\n",
            "-------TRAIN_LOSS:  18.2822\n",
            "EPOCH: 90\n",
            "-------TRAIN_LOSS:  17.6315\n",
            "EPOCH: 95\n",
            "-------TRAIN_LOSS:  21.2692\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generation"
      ],
      "metadata": {
        "id": "UOv-8BIQa3mW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(seeded_text,temperature,max_len=10):\n",
        "    model.eval()\n",
        "    if seeded_text is None: \n",
        "      seeded_text = \"<bos>\"\n",
        "    BATCH_SIZE = 1\n",
        "    tokens = text_pipeline(seeded_text)\n",
        "    hidden, cell = model.init_hidden_and_cell(BATCH_SIZE)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for i in range(max_len):\n",
        "        input = torch.tensor(tokens,device=device).unsqueeze(0)\n",
        "        prediction, hidden, cell = model(input, hidden, cell)\n",
        "        probs = torch.softmax(prediction[:, -1]/temperature, dim=-1) \n",
        "        prediction = torch.multinomial(probs, num_samples=1).item()\n",
        "        tokens.append(prediction)\n",
        "\n",
        "    tokens = [Vocab.get_itos()[i] for i in tokens]\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "V8O1iFQhn65x"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "' '.join(generate(\"they thought\",temperature=0.5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "r905FKI0UYCh",
        "outputId": "408e52ce-7865-4c59-85f9-b21708045540"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'they thought to him what sort of thing can t happen it'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "' '.join(generate(None,temperature=1.5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mAYUkp1vUx-0",
        "outputId": "bbb1fc81-46ae-4d96-e139-a23fec1e852e"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the same boat we might light a bonfire tonight lombard said'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "' '.join(generate(\"we must\",temperature=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CRTuTpLYS6FP",
        "outputId": "c6294751-2c4f-4913-941d-fd71cbd024d0"
      },
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'we must enjoy a sensible young man where do we warn quickly'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 182
        }
      ]
    }
  ]
}